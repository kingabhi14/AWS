{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions\n"
      ],
      "metadata": {
        "id": "8Ibc0PMH7E7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for\n",
        "data analysis and latency-sensitive applications\"\n",
        "\n",
        " 1. AWS Regions\n",
        "\n",
        "Definition:\n",
        "A Region is a geographically distinct location that contains multiple Availability Zones. AWS has Regions all over the world (e.g., US East (N. Virginia), Europe (Frankfurt), Asia Pacific (Tokyo)).\n",
        "\n",
        "Purpose:\n",
        "Regions allow you to deploy resources close to your users or data sources. Each Region is isolated from others to provide fault tolerance and data sovereignty.\n",
        "\n",
        "Significance:\n",
        "Choosing the right Region affects data residency (important for compliance), latency, and disaster recovery strategies.\n",
        "\n",
        "2. Availability Zones (AZs)\n",
        "\n",
        "Definition:\n",
        "An Availability Zone is a physically separate data center within a Region. Each Region has 2 or more AZs.\n",
        "\n",
        "Purpose:\n",
        "AZs are designed for fault isolation. They are connected with low-latency, high-bandwidth networks but are physically independent to prevent a single point of failure.\n",
        "\n",
        "Significance:\n",
        "You can deploy your application across multiple AZs for high availability and fault tolerance. For example, if one AZ goes down, another can serve traffic seamlessly.\n",
        "\n",
        "3. Edge Locations\n",
        "\n",
        "Definition:\n",
        "Edge Locations are sites used by AWS for caching content closer to end users via the AWS Content Delivery Network (CDN) service called Amazon CloudFront.\n",
        "\n",
        "Purpose:\n",
        "They store cached copies of static or dynamic content to reduce latency by serving requests from the nearest location to the user.\n",
        "\n",
        "Significance:\n",
        "Edge Locations are important for accelerating content delivery and API responses globally. They are not designed for hosting or running applications but for caching and delivering content quickly.\n",
        "\n"
      ],
      "metadata": {
        "id": "-eNFw5Cu7PWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.  Using the AWS CLI, list all available AWS regions. Share the command used and the output&\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9erinrXQ77RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aws ec2 describe-regions --query \"Regions[].RegionName\" --output text\n",
        "\n",
        "OUTPUT : us-east-1 us-east-2 us-west-1 us-west-2 af-south-1 ap-east-1 ap-south-1 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-southeast-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 eu-west-3 eu-north-1 eu-south-1 me-south-1 sa-east-1\n"
      ],
      "metadata": {
        "id": "lMxDri3w8Ccs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Create a new IAM user with least privilege access to Amazon S3. Share your attached policies (JSON or\n",
        "screenshot)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BdEdtnHv8l4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:ListAllMyBuckets\"\n",
        "            ],\n",
        "            \"Resource\": \"arn:aws:s3:::*\"\n",
        "        },\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:GetBucketLocation\",\n",
        "                \"s3:ListBucket\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                \"arn:aws:s3:::*\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:GetObject\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                \"arn:aws:s3:::*/*\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "fjiLStIr898-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 . Compare different Amazon S3 storage (Standard, Intelligent-Tiering, Glacier). When should each be used in\n",
        "data analytics workflows\"\n",
        "\n",
        "1. Amazon S3 Standard\n",
        "Description:\n",
        "The default storage class designed for frequently accessed data. Offers high durability, availability, and low latency.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Millisecond access latency\n",
        "\n",
        "99.99% availability\n",
        "\n",
        "High throughput and performance\n",
        "\n",
        "Suitable for frequently accessed, critical data\n",
        "\n",
        "Use Cases in Data Analytics:\n",
        "\n",
        "Storing raw data ingested from sources for immediate processing\n",
        "\n",
        "Storing active datasets and intermediate results that analytics jobs query repeatedly\n",
        "\n",
        "Real-time analytics where low latency is critical\n",
        "\n",
        "2. Amazon S3 Intelligent-Tiering\n",
        "Description:\n",
        "Automatically moves data between two tiers: frequent access and infrequent access, based on changing access patterns, without performance impact or operational overhead.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Automatic cost optimization based on access patterns\n",
        "\n",
        "No retrieval fees or operational complexity\n",
        "\n",
        "Designed for datasets with unknown or unpredictable access patterns\n",
        "\n",
        "Slightly higher storage cost than Standard but saves money on infrequently accessed data\n",
        "\n",
        "Use Cases in Data Analytics:\n",
        "\n",
        "Datasets with unpredictable or fluctuating access patterns\n",
        "\n",
        "Data that might be frequently accessed initially but becomes infrequently accessed later\n",
        "\n",
        "Long-term storage where access patterns are uncertain, such as logs or intermediate analysis outputs that might be needed later\n",
        "\n",
        "3. Amazon S3 Glacier (and Glacier Deep Archive)\n",
        "Description:\n",
        "Designed for long-term archival and backup with very low storage costs but higher retrieval latency.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Retrieval times range from minutes (Glacier Instant Retrieval) to hours (Standard and Deep Archive)\n",
        "\n",
        "Very low storage costs\n",
        "\n",
        "Suitable for data that is rarely accessed but must be retained\n",
        "\n",
        "Use Cases in Data Analytics:\n",
        "\n",
        "Archiving historical datasets that are no longer actively queried but must be retained for compliance or future analysis\n",
        "\n",
        "Storing old raw data before deletion or for audit purposes\n",
        "\n",
        "Backup of analytics data that is costly to regenerate but rarely accessed\n",
        "\n",
        "Summary Table\n",
        "Storage Class\tAccess Frequency\tRetrieval Latency\tCost\tUse Case in Data Analytics\n",
        "S3 Standard\tFrequent\tMilliseconds\tHighest among these\tActive datasets, real-time analytics\n",
        "S3 Intelligent-Tiering\tVariable/Unpredictable\tMilliseconds (frequent tier)\tModerate\tDatasets with unknown access patterns\n",
        "S3 Glacier\tRare/Archive\tMinutes to hours\tLowest\tLong-term archival, historical data storage"
      ],
      "metadata": {
        "id": "UAUud4am8_5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write and apply a lifecycle policy to move files to Glacier after 30 days and delete them after 90. Share the\n",
        "policy JSON or Screenshot&\n",
        "\n"
      ],
      "metadata": {
        "id": "PbxG7Z1I9Xlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierAndDelete\",\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Filter\": {\n",
        "        \"Prefix\": \"\"\n",
        "      },\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      },\n",
        "      \"NoncurrentVersionExpiration\": {\n",
        "        \"NoncurrentDays\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "ET_hCiiU9iHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare RDS, DynamoDB, and Redshift for use in different stages of a data pipeline. Give one use case for\n",
        "each&\n",
        "\n",
        "1. Amazon RDS (Relational Database Service)\n",
        "Type: Managed relational database (supports engines like MySQL, PostgreSQL, SQL Server, etc.)\n",
        "\n",
        "Best for: OLTP (Online Transaction Processing), structured data, complex queries with relational joins\n",
        "\n",
        "Data Pipeline Stage: Data ingestion & transactional storage\n",
        "RDS is great for capturing and storing structured data in real time from applications or data sources before moving it further down the pipeline.\n",
        "\n",
        "Use Case:\n",
        "Storing user profile data and transactions in an e-commerce app before analytics.\n",
        "Example: Orders, payments, and customer data stored in RDS to ensure ACID compliance and support complex transactional queries.\n",
        "\n",
        "2. Amazon DynamoDB\n",
        "Type: Fully managed NoSQL key-value and document database\n",
        "\n",
        "Best for: Highly scalable, low-latency applications with flexible schema, event-driven data ingestion\n",
        "\n",
        "Data Pipeline Stage: Real-time data ingestion and quick lookups\n",
        "Ideal for fast ingestion of semi-structured or unstructured data, with millisecond response times.\n",
        "\n",
        "Use Case:\n",
        "Capturing real-time IoT sensor data streams.\n",
        "Example: Storing time-series sensor data that needs rapid ingestion and fast retrieval for dashboarding or quick anomaly detection.\n",
        "\n",
        "3. Amazon Redshift\n",
        "Type: Fully managed data warehouse (columnar storage, optimized for OLAP workloads)\n",
        "\n",
        "Best for: Large-scale analytics, complex queries, data aggregation, and reporting\n",
        "\n",
        "Data Pipeline Stage: Data storage & analytics\n",
        "Used as the central analytics platform for running complex, large-scale queries on structured data aggregated from various sources.\n",
        "\n",
        "Use Case:\n",
        "Analyzing customer behavior and sales trends using aggregated data from RDS and DynamoDB.\n",
        "Example: Running complex BI queries and reports on historical sales data, customer segmentation, and marketing effectiveness."
      ],
      "metadata": {
        "id": "06-UAnKQ9j5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.  Create a DynamoDB table and insert 3 records manually. Then write a Lambda function that adds records\n",
        "when triggered by S3 uploads&\n",
        "\n",
        " Create a DynamoDB Table\n",
        "Let's create a simple table called Uploads with UploadId as the partition key.\n",
        "\n",
        "Using AWS CLI:\n"
      ],
      "metadata": {
        "id": "k2k7cPn69w4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aws dynamodb create-table \\\n",
        "    --table-name Uploads \\\n",
        "    --attribute-definitions AttributeName=UploadId,AttributeType=S \\\n",
        "    --key-schema AttributeName=UploadId,KeyType=HASH \\\n",
        "    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n"
      ],
      "metadata": {
        "id": "PhUnv7ID-DWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Insert 3 records manually\n",
        "Using AWS CLI put-item command to add three sample items:"
      ],
      "metadata": {
        "id": "gLZpqI4X-ENj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aws dynamodb put-item --table-name Uploads --item '{\"UploadId\": {\"S\": \"upload1\"}, \"FileName\": {\"S\": \"file1.csv\"}, \"UploadTime\": {\"S\": \"2025-06-22T10:00:00Z\"}}'\n",
        "\n",
        "aws dynamodb put-item --table-name Uploads --item '{\"UploadId\": {\"S\": \"upload2\"}, \"FileName\": {\"S\": \"file2.csv\"}, \"UploadTime\": {\"S\": \"2025-06-22T11:00:00Z\"}}'\n",
        "\n",
        "aws dynamodb put-item --table-name Uploads --item '{\"UploadId\": {\"S\": \"upload3\"}, \"FileName\": {\"S\": \"file3.csv\"}, \"UploadTime\": {\"S\": \"2025-06-22T12:00:00Z\"}}'\n"
      ],
      "metadata": {
        "id": "_PnuWO6u-Gua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Write a Lambda function triggered by S3 uploads to add records\n",
        "Lambda Function (Python)\n",
        "This function will trigger on S3 ObjectCreated events and insert a record into the DynamoDB table."
      ],
      "metadata": {
        "id": "yfUmIWEW-Iin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "dynamodb = boto3.resource('dynamodb')\n",
        "table = dynamodb.Table('Uploads')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        s3_info = record['s3']\n",
        "        bucket_name = s3_info['bucket']['name']\n",
        "        object_key = s3_info['object']['key']\n",
        "\n",
        "        upload_id = str(uuid.uuid4())\n",
        "\n",
        "        upload_time = datetime.utcnow().isoformat() + 'Z'\n",
        "\n",
        "\n",
        "        table.put_item(\n",
        "            Item={\n",
        "                'UploadId': upload_id,\n",
        "                'FileName': object_key,\n",
        "                'BucketName': bucket_name,\n",
        "                'UploadTime': upload_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Upload records added to DynamoDB')\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KVqq8RfX-M4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is serverless computing? Discuss pros and cons of using AWS Lambda for data pipelines&\n",
        "\n",
        "**Serverless computing: **\n",
        "\n",
        "Serverless computing is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers.\n",
        "\n",
        "You don’t have to manage infrastructure (no servers or VMs to provision or maintain).\n",
        "\n",
        "You write code/functions that are triggered by events (like file uploads, database changes, API calls).\n",
        "\n",
        "You pay only for the compute time you consume—no charges when your code isn't running.\n",
        "\n",
        "It’s highly scalable and event-driven.\n",
        "\n",
        "AWS Lambda is one of the most popular serverless compute services.\n",
        "\n",
        "Pros of Using AWS Lambda for Data Pipelines\n",
        "No Server Management:\n",
        "You focus on your code and logic; AWS handles the underlying infrastructure.\n",
        "\n",
        "Automatic Scalability:\n",
        "Lambda automatically scales to handle the volume of events, so it adapts well to fluctuating data loads.\n",
        "\n",
        "Cost-Effective:\n",
        "Pay only for actual execution time (in milliseconds), no cost when idle.\n",
        "\n",
        "Event-Driven Integration:\n",
        "Lambda natively integrates with many AWS services (S3, DynamoDB, Kinesis, SNS, etc.), which is great for building reactive data pipelines.\n",
        "\n",
        "Rapid Development & Deployment:\n",
        "Small, modular functions mean faster iterations and easier debugging.\n",
        "\n",
        "Built-in Fault Tolerance:\n",
        "AWS retries failed executions automatically, improving reliability.\n",
        "\n",
        "Cons of Using AWS Lambda for Data Pipelines\n",
        "Execution Time Limits:\n",
        "Lambda has a max execution time of 15 minutes, which is limiting for long-running ETL or complex batch processes.\n",
        "\n",
        "Cold Start Latency:\n",
        "Functions may experience latency when they’re invoked after a period of inactivity (especially for larger functions or in VPC).\n",
        "\n",
        "Resource Limitations:\n",
        "Memory max is 10 GB, disk space (512 MB /tmp), and ephemeral storage constraints can restrict certain workloads.\n",
        "\n",
        "Complexity in Orchestration:\n",
        "Managing complex workflows with multiple functions and retries requires additional services like AWS Step Functions, adding complexity.\n",
        "\n",
        "Limited Language Support:\n",
        "While AWS Lambda supports many languages, some niche or legacy languages may not be supported directly.\n",
        "\n",
        "Monitoring and Debugging Challenges:\n",
        "Distributed functions can make tracing and debugging more complex compared to monolithic applications."
      ],
      "metadata": {
        "id": "VQ5f3ZUZ-ffP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Create a Lambda function triggered by S3 uploads that logs file name, size, and timestamp to Cloudwatch.\n",
        "Share code and a log screenshot\n",
        "\n"
      ],
      "metadata": {
        "id": "zCAQ-ypD-6Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "\n",
        "    for record in event['Records']:\n",
        "        s3 = record['s3']\n",
        "        bucket_name = s3['bucket']['name']\n",
        "        object_key = s3['object']['key']\n",
        "        object_size = s3['object']['size']\n",
        "\n",
        "\n",
        "        timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "\n",
        "        logger.info(f\"File uploaded: {object_key}\")\n",
        "        logger.info(f\"Bucket: {bucket_name}\")\n",
        "        logger.info(f\"Size (bytes): {object_size}\")\n",
        "        logger.info(f\"Upload timestamp: {timestamp}\")\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Logged S3 upload details successfully.')\n",
        "    }\n"
      ],
      "metadata": {
        "id": "HJ8NwdRx_LVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 .  Explain the difference between Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics. Provide a\n",
        "real-world example of how each would be used\n",
        "\n",
        "1. Kinesis Data Streams\n",
        "What it is:\n",
        "A scalable, real-time data streaming service that allows you to capture and store data streams from multiple sources for processing.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "You build custom applications (consumers) to process data in real-time.\n",
        "\n",
        "Data retention can be from 24 hours up to 7 days (or longer with extended retention).\n",
        "\n",
        "Low latency ingestion and retrieval.\n",
        "\n",
        "Real-world Example:\n",
        "Collecting and processing clickstream data from a high-traffic website.\n",
        "Example: A gaming company collects player actions in real-time to power leaderboards and in-game analytics.\n",
        "\n",
        "2. Kinesis Data Firehose\n",
        "What it is:\n",
        "A fully managed service for loading streaming data into data stores and analytics tools (like S3, Redshift, Elasticsearch, Splunk).\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "No need to write custom consumer applications.\n",
        "\n",
        "Supports automatic data transformation (via AWS Lambda) and compression.\n",
        "\n",
        "Near real-time data delivery with automatic scaling.\n",
        "\n",
        "Real-world Example:\n",
        "Automatically ingesting IoT sensor data into Amazon S3 for long-term storage and batch analytics.\n",
        "Example: A manufacturing plant streams sensor data and Firehose delivers it directly to S3, optionally transforming and compressing it on the fly.\n",
        "\n",
        "3. Kinesis Data Analytics\n",
        "What it is:\n",
        "Enables real-time processing and analytics of streaming data using standard SQL queries.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "No need to write complex code; use SQL to analyze streaming data.\n",
        "\n",
        "Integrates with Kinesis Data Streams and Firehose as data sources and sinks.\n",
        "\n",
        "Useful for real-time dashboards, anomaly detection, and filtering.\n",
        "\n",
        "Real-world Example:\n",
        "Monitoring social media sentiment in real-time during a marketing campaign.\n",
        "Example: Data from Kinesis Data Streams is fed into Kinesis Data Analytics to identify spikes in positive or negative sentiment instantly."
      ],
      "metadata": {
        "id": "p5mg0U1A_P_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.  What is columnar storage and how does it benefit Redshift performance for analytics workloads\"\n",
        "\n",
        "Columnar storage is a way of organizing data in a database where data is stored column-by-column instead of row-by-row.\n",
        "\n",
        "Instead of storing full rows one after another (row-oriented), columnar storage stores all the values of a single column together.\n",
        "\n",
        "For example, all values from the \"age\" column are stored sequentially, then all values from the \"salary\" column, and so on.\n",
        "\n",
        "Amazon Redshift is a columnar data warehouse, and this storage format gives it several advantages for analytics workloads:\n",
        "\n",
        "1. Faster Query Performance\n",
        "Analytical queries usually involve aggregations and scans over a subset of columns (e.g., sum of sales, average age).\n",
        "\n",
        "Columnar storage allows Redshift to read only the columns required for the query, minimizing the amount of data read from disk.\n",
        "\n",
        "2. Better Compression\n",
        "Since columns contain similar data types and often similar values, Redshift can apply highly effective compression algorithms.\n",
        "\n",
        "This reduces storage space and speeds up I/O by reading less data.\n",
        "\n",
        "3. Efficient Data Scanning\n",
        "Scanning a few columns is faster because the database skips irrelevant columns entirely.\n",
        "\n",
        "This leads to lower I/O, CPU usage, and faster query response times.\n",
        "\n",
        "4. Optimized for OLAP Workloads\n",
        "Redshift and other data warehouses are designed for Online Analytical Processing (OLAP), which involves large-scale aggregations and reporting rather than frequent single-row transactions.\n",
        "\n",
        "Columnar storage matches these patterns perfectly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AAD8oh-Z_cIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. What is the role of the AWS Glue Data Catalog in Athena? How does schema-on-read work?\n",
        "\n",
        "Role of AWS Glue Data Catalog in Athena\n",
        "AWS Glue Data Catalog is a central metadata repository — it stores information about data sources like tables, schemas, partitions, and their locations.\n",
        "\n",
        "When you run queries in Amazon Athena (which is a serverless interactive query service for data in S3), Athena uses the Glue Data Catalog as its metadata store to understand:\n",
        "\n",
        "What tables exist,\n",
        "\n",
        "What columns they have,\n",
        "\n",
        "What data types each column is,\n",
        "\n",
        "Where the data physically lives in S3.\n",
        "\n",
        "Essentially, the Glue Data Catalog tells Athena how to interpret the raw data files stored in S3 without needing to move or transform the data first.\n",
        "\n",
        "How Schema-on-Read Works\n",
        "Unlike traditional databases that enforce a schema when you write data (schema-on-write), services like Athena and Glue use schema-on-read.\n",
        "\n",
        "Schema-on-read means:\n",
        "\n",
        "You store raw data in S3 (e.g., CSV, JSON, Parquet) without enforcing a schema upfront.\n",
        "\n",
        "When you run a query, Athena applies the schema at query time, based on metadata from the Glue Data Catalog.\n",
        "\n",
        "The query engine reads the raw data and interprets it according to the schema defined in the Data Catalog.\n",
        "\n",
        "This allows flexibility:\n",
        "\n",
        "You can store different data formats without upfront conversion.\n",
        "\n",
        "You can evolve schemas without rewriting data.\n",
        "\n",
        "You avoid the overhead of schema validation on data ingestion."
      ],
      "metadata": {
        "id": "C4zmTSXk_twZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. Create an Athena table from S3 data using Glue Catalog. Run a query and share the SQL + result screenshot\n",
        "\n",
        "SELECT column_name, COUNT(*) AS count\n",
        "FROM my_database.my_table\n",
        "GROUP BY column_name\n",
        "ORDER BY count DESC\n",
        "LIMIT 5;\n"
      ],
      "metadata": {
        "id": "pQeOrBCP_4af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 17. Describe how Amazon Quicksight supports business intelligence in a serverless data architecture. What are\n",
        "SPICE and embedded dashboards\n",
        "\n",
        "Amazon QuickSight and Serverless BI\n",
        "Amazon QuickSight is a fully managed, serverless business intelligence (BI) service that enables organizations to easily create and publish interactive dashboards and visualizations without managing any infrastructure.\n",
        "\n",
        "Serverless Architecture:\n",
        "You don’t provision or manage servers; QuickSight automatically scales to handle users and data size.\n",
        "\n",
        "Direct Integration with AWS Data Sources:\n",
        "QuickSight connects directly to services like Athena, Redshift, RDS, S3, and Glue Data Catalog, making it a natural fit for serverless data lakes and pipelines.\n",
        "\n",
        "Fast, Scalable BI:\n",
        "Users can quickly explore and visualize large datasets with minimal latency and effort.\n",
        "\n",
        "What is SPICE?\n",
        "SPICE (Super-fast, Parallel, In-memory Calculation Engine) is QuickSight’s proprietary, in-memory data engine designed for fast data querying and visualization.\n",
        "\n",
        "How it works:\n",
        "SPICE imports data from your sources into a highly optimized, compressed, and parallelized in-memory engine.\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Speed: Queries run faster because data is pre-loaded in-memory.\n",
        "\n",
        "Scalability: Can handle millions of rows and hundreds of concurrent users.\n",
        "\n",
        "Offline Access: Users can interact with dashboards without querying the underlying source each time.\n",
        "\n",
        "Cost-Efficient: Reduces load on the original data sources.\n",
        "\n",
        "What are Embedded Dashboards?\n",
        "Embedded Dashboards allow you to integrate QuickSight visualizations directly into your own applications, portals, or websites.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Provide analytics to customers without requiring them to log into QuickSight separately.\n",
        "\n",
        "Build custom BI experiences with your app’s branding and user management.\n",
        "\n",
        "How it works:\n",
        "QuickSight generates secure URLs or iFrames with embedded dashboards that your app can render seamlessly.\n",
        "\n"
      ],
      "metadata": {
        "id": "jYlZg0dMAgZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19 . Explain how AWS CloudWatch and CloudTrail differ. IN a data analytics pipeline, what role does each play in\n",
        "monitoring, auditing, and troubleshooting?\n",
        "\n",
        "AWS CloudWatch vs AWS CloudTrail: What’s the Difference?\n",
        "Aspect\tAWS CloudWatch\tAWS CloudTrail\n",
        "Purpose\tMonitoring and observability of AWS resources and applications\tGovernance, compliance, auditing, and operational auditing\n",
        "Data Type\tMetrics, logs, events from AWS services & apps\tAPI call history (who did what, when, where)\n",
        "Focus\tReal-time system performance and operational health\tTracking changes and user activity\n",
        "Examples\tCPU usage, memory utilization, application logs, alarms\tUser logins, API calls like CreateBucket, RunInstances\n",
        "\n",
        "Role in a Data Analytics Pipeline\n",
        "Function\tAWS CloudWatch\tAWS CloudTrail\n",
        "Monitoring\t- Track pipeline health (e.g., job success/failure, latency)\n",
        "- Monitor resource utilization (EC2, Lambda, Redshift)\n",
        "- Trigger alarms on anomalies\t- Not used directly for monitoring resource health\n",
        "Auditing\t- Limited audit capability (log storage only)\t- Full audit trail of all API calls and user actions\n",
        "- Essential for compliance and forensic analysis\n",
        "Troubleshooting\t- Analyze application and infrastructure logs\n",
        "- Visualize metrics to detect bottlenecks or failures\n",
        "- Create dashboards for operational visibility\t- Investigate unauthorized or unintended changes\n",
        "- Trace who initiated failed or unexpected actions"
      ],
      "metadata": {
        "id": "FZ7Pa3GiAzxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. Describe a complete end-to-end data analytics pipeline using AWS services. Include services for data\n",
        "ingestion, storage, transformation, querying, and visualization. (Example: S3 → Lambda → Glue → Quicksight)\n",
        "Explain why you would choose each service for the stage it’s used in?\n",
        "\n",
        "Example Pipeline:\n",
        "Data Ingestion → Storage → Transformation → Querying → Visualization\n",
        "\n",
        "1. Data Ingestion: Amazon Kinesis Data Firehose\n",
        "Why?\n",
        "\n",
        "Seamlessly ingests streaming data from sources like IoT devices, applications, or logs.\n",
        "\n",
        "Fully managed, scales automatically, and delivers data reliably to destinations like S3.\n",
        "\n",
        "Supports data transformation via Lambda during ingestion (e.g., format conversion).\n",
        "\n",
        "2. Storage: Amazon S3\n",
        "Why?\n",
        "\n",
        "Highly durable, scalable, and cost-effective object storage.\n",
        "\n",
        "Ideal for storing raw, processed, and archived datasets of any size.\n",
        "\n",
        "Serves as the “data lake” that centralizes all incoming data.\n",
        "\n",
        "3. Transformation: AWS Glue\n",
        "Why?\n",
        "\n",
        "Serverless ETL service that crawls data to infer schemas, catalog data, and orchestrate transformations.\n",
        "\n",
        "Supports Python/Scala scripts for flexible data processing (e.g., converting CSV to Parquet, cleaning).\n",
        "\n",
        "Integrates directly with S3, Glue Data Catalog, and Athena for smooth downstream querying.\n",
        "\n",
        "4. Querying: Amazon Athena\n",
        "Why?\n",
        "\n",
        "Serverless interactive query service that uses SQL to analyze data directly in S3.\n",
        "\n",
        "No infrastructure to manage, scales automatically, and you pay per query.\n",
        "\n",
        "Uses Glue Data Catalog for metadata, enabling schema-on-read querying without data movement.\n",
        "\n",
        "5. Visualization: Amazon QuickSight\n",
        "Why?\n",
        "\n",
        "Serverless BI tool with fast, interactive dashboards and built-in ML insights.\n",
        "\n",
        "Connects directly to Athena, Redshift, and other data sources.\n",
        "\n",
        "Supports SPICE for fast in-memory analysis and embedding dashboards in apps."
      ],
      "metadata": {
        "id": "C6qKW_SVBWv2"
      }
    }
  ]
}